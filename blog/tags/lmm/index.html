<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v0.0.0-4193">
<link rel="alternate" type="application/rss+xml" href="/autogen/blog/rss.xml" title="AutoGen RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/autogen/blog/atom.xml" title="AutoGen Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous"><title data-react-helmet="true">One post tagged with &quot;LMM&quot; | AutoGen</title><meta data-react-helmet="true" property="og:title" content="One post tagged with &quot;LMM&quot; | AutoGen"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://microsoft.github.io/autogen/blog/tags/lmm"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_tag" content="blog_tags_posts"><link data-react-helmet="true" rel="shortcut icon" href="/autogen/img/ag.ico"><link data-react-helmet="true" rel="canonical" href="https://microsoft.github.io/autogen/blog/tags/lmm"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io/autogen/blog/tags/lmm" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://microsoft.github.io/autogen/blog/tags/lmm" hreflang="x-default"><link rel="stylesheet" href="/autogen/assets/css/styles.a35b243d.css">
<link rel="preload" href="/autogen/assets/js/runtime~main.c3739084.js" as="script">
<link rel="preload" href="/autogen/assets/js/main.8b4aabfe.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/autogen/"><div class="navbar__logo"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedImage_TMUO themedImage--light_4Vu1"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedImage_TMUO themedImage--dark_uzRr"></div><b class="navbar__title">AutoGen</b></a><a class="navbar__item navbar__link" href="/autogen/docs/Getting-Started">Docs</a><a class="navbar__item navbar__link" href="/autogen/docs/reference/agentchat/conversable_agent">SDK</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/autogen/blog">Blog</a><a class="navbar__item navbar__link" href="/autogen/docs/FAQ">FAQ</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/microsoft/autogen" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_iYfV toggle_2i4l toggleDisabled_xj38"><div class="toggleTrack_t-f2" role="button" tabindex="-1"><div class="toggleTrackCheck_mk7D"><span class="toggleIcon_pHJ9">ðŸŒœ</span></div><div class="toggleTrackX_dm8H"><span class="toggleIcon_pHJ9">ðŸŒž</span></div><div class="toggleTrackThumb_W6To"></div></div><input type="checkbox" class="toggleScreenReader_h9qa" aria-label="Switch between dark and light mode"></div><div class="navbar__search searchBarContainer_I7kZ"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_Zg7X searchBarLoadingRing_J5Ez"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_CDc6"><kbd class="searchHint_2RRg">ctrl</kbd><kbd class="searchHint_2RRg">K</kbd></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_q+wC thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_9G5K margin-bottom--md">Recent posts</div><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/11/06/LMM-Agent">Multimodal with GPT-4V and LLaVA</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/10/26/TeachableAgent">AutoGen&#x27;s TeachableAgent</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/10/18/RetrieveChat">Retrieval-Augmented Generation (RAG) Applications with AutoGen</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/07/14/Local-LLMs">Use AutoGen for Local LLMs</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/autogen/blog/2023/06/28/MathChat">MathChat - An Conversational Framework to Solve Math Problems</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>One post tagged with &quot;LMM&quot;</h1><a href="/autogen/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_d4p0" itemprop="headline"><a itemprop="url" href="/autogen/blog/2023/11/06/LMM-Agent">Multimodal with GPT-4V and LLaVA</a></h2><div class="blogPostData_-Im+ margin-vert--md"><time datetime="2023-11-06T00:00:00.000Z" itemprop="datePublished">November 6, 2023</time> Â· <!-- -->3 min read</div><div class="row margin-top--md margin-bottom--sm"><div class="col col--6 authorCol_8c0z"><div class="avatar margin-bottom--sm"><a href="https://github.com/beibinli" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img class="image_9q7L" src="https://github.com/beibinli.png" alt="Beibin Li"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/beibinli" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Beibin Li</span></a></div><small class="avatar__subtitle" itemprop="description">Senior Research Engineer at Microsoft</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img alt="LMM Teaser" src="/autogen/assets/images/teaser-380bdaa90a1c02ad009520bf289776c9.png"></p><p><strong>In Brief:</strong></p><ul><li>Introducing the <strong>Multimodal Conversable Agent</strong> and the <strong>LLaVA Agent</strong> to enhance LMM functionalities.</li><li>Users can input text and images simultaneously using the <code>&lt;img img_path&gt;</code> tag to specify image loading.</li><li>Demonstrated through the <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb" target="_blank" rel="noopener noreferrer">GPT-4V notebook</a>.</li><li>Demonstrated through the <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb" target="_blank" rel="noopener noreferrer">LLaVA notebook</a>.</li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="introduction">Introduction<a aria-hidden="true" class="hash-link" href="#introduction" title="Direct link to heading">â€‹</a></h2><p>Large multimodal models (LMMs) augment large language models (LLMs) with the ability to process multi-sensory data.</p><p>This blog post and the latest AutoGen update concentrate on visual comprehension. Users can input images, pose questions about them, and receive text-based responses from these LMMs.
We support the <code>gpt-4-vision-preview</code> model from OpenAI and <code>LLaVA</code> model from Microsoft now.</p><p>Here, we emphasize the <strong>Multimodal Conversable Agent</strong> and the <strong>LLaVA Agent</strong> due to their growing popularity.
GPT-4V represents the forefront in image comprehension, while LLaVA is an efficient model, fine-tuned from LLama-2.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="installation">Installation<a aria-hidden="true" class="hash-link" href="#installation" title="Direct link to heading">â€‹</a></h2><p>Incorporate the <code>lmm</code> feature during AutoGen installation:</p><div class="codeBlockContainer_J+bg language-bash"><div class="codeBlockContent_csEI bash"><pre tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">pip </span><span class="token function" style="color:rgb(130, 170, 255)">install</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;pyautogen[lmm]&quot;</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Subsequently, import the <strong>Multimodal Conversable Agent</strong> or <strong>LLaVA Agent</strong> from AutoGen:</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">agentchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">contrib</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">multimodal_conversable_agent </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> MultimodalConversableAgent  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># for GPT-4V</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">agentchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">contrib</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">llava_agent </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> LLaVAAgent  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># for LLaVA</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h2 class="anchor anchorWithStickyNavbar_y2LR" id="usage">Usage<a aria-hidden="true" class="hash-link" href="#usage" title="Direct link to heading">â€‹</a></h2><p>A simple syntax has been defined to incorporate both messages and images within a single string.</p><p>Example of an in-context learning prompt:</p><div class="codeBlockContainer_J+bg language-python"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#bfc7d5"><span class="token plain">prompt </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;You are now an image classifier for facial expressions. Here are</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">some examples.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(195, 232, 141)"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&lt;img happy.jpg&gt; depicts a happy expression.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&lt;img http://some_location.com/sad.jpg&gt; represents a sad expression.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&lt;img obama.jpg&gt; portrays a neutral expression.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(195, 232, 141)"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">Now, identify the facial expression of this individual: &lt;img unknown.png&gt;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">agent </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> MultimodalConversableAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">user </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> UserProxyAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">user</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">initiate_chat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">agent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">prompt</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>The <code>MultimodalConversableAgent</code> interprets the input prompt, extracting images from local or internet sources.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="advanced-usage">Advanced Usage<a aria-hidden="true" class="hash-link" href="#advanced-usage" title="Direct link to heading">â€‹</a></h2><p>Similar to other AutoGen agents, multimodal agents support multi-round dialogues with other agents, code generation, factual queries, and management via a GroupChat interface.</p><p>For example, the <code>FigureCreator</code> in our <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb" target="_blank" rel="noopener noreferrer">GPT-4V notebook</a> and <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb" target="_blank" rel="noopener noreferrer">LLaVA notebook</a> integrates two agents: a coder (an AssistantAgent) and critics (a multimodal agent).
The coder drafts Python code for visualizations, while the critics provide insights for enhancement. Collaboratively, these agents aim to refine visual outputs.
With <code>human_input_mode=ALWAYS</code>, you can also contribute suggestions for better visualizations.</p><h2 class="anchor anchorWithStickyNavbar_y2LR" id="reference">Reference<a aria-hidden="true" class="hash-link" href="#reference" title="Direct link to heading">â€‹</a></h2><ul><li><a href="https://openai.com/research/gpt-4v-system-card" target="_blank" rel="noopener noreferrer">GPT-4V System Card</a></li><li><a href="https://github.com/haotian-liu/LLaVA" target="_blank" rel="noopener noreferrer">LLaVA GitHub</a></li></ul><h2 class="anchor anchorWithStickyNavbar_y2LR" id="future-enhancements">Future Enhancements<a aria-hidden="true" class="hash-link" href="#future-enhancements" title="Direct link to heading">â€‹</a></h2><p>For further inquiries or suggestions, please open an issue in the <a href="https://github.com/microsoft/autogen/" target="_blank" rel="noopener noreferrer">AutoGen repository</a> or contact me directly at <a href="mailto:beibin.li@microsoft.com" target="_blank" rel="noopener noreferrer">beibin.li@microsoft.com</a>.</p><p>AutoGen will continue to evolve, incorporating more multimodal functionalities such as DALLE model integration, audio interaction, and video comprehension. Stay tuned for these exciting developments.</p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_NBRY padding--none margin-left--sm"><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/autogen/blog/tags/lmm">LMM</a></li><li class="tag_F03v"><a class="tag_WK-t tagRegular_LXbV" href="/autogen/blog/tags/multimodal">multimodal</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Multimodal with GPT-4V and LLaVA" href="/autogen/blog/2023/11/06/LMM-Agent"><b>Read More</b></a></div></footer></article></main></div></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://discord.gg/pAbnFJrkgZ" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li><li class="footer__item"><a href="https://twitter.com/pyautogen" target="_blank" rel="noopener noreferrer" class="footer__link-item"><span>Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023 AutoGen Authors.</div></div></div></footer></div>
<script src="/autogen/assets/js/runtime~main.c3739084.js"></script>
<script src="/autogen/assets/js/main.8b4aabfe.js"></script>
</body>
</html>