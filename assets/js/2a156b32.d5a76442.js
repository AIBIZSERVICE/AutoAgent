"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[9829],{5533:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"AutoGen\'s TeachableAgent","metadata":{"permalink":"/autogen/blog/2023/10/26/TeachableAgent","source":"@site/blog/2023-10-26-TeachableAgent/index.mdx","title":"AutoGen\'s TeachableAgent","description":"Teachable Agent Architecture","date":"2023-10-26T00:00:00.000Z","formattedDate":"October 26, 2023","tags":[{"label":"LLM","permalink":"/autogen/blog/tags/llm"},{"label":"teach","permalink":"/autogen/blog/tags/teach"}],"readingTime":16.715,"truncated":false,"authors":[{"name":"Ricky Loynd","title":"Senior Research Engineer at Microsoft","url":"https://github.com/rickyloynd-microsoft","imageURL":"https://github.com/rickyloynd-microsoft.png","key":"rickyloynd-microsoft"}],"nextItem":{"title":"Retrieval-Augmented Generation (RAG) Applications with AutoGen","permalink":"/autogen/blog/2023/10/18/RetrieveChat"}},"content":"![Teachable Agent Architecture](img/teachable-arch.png)\\n\\n**TL;DR:**\\n* We introduce **TeachableAgent** (which uses **TextAnalyzerAgent**) so that users can teach their LLM-based assistants new facts, preferences, and skills.\\n* We showcase examples of `TeachableAgent` learning and later recalling facts, preferences, and skills in subsequent chats.\\n\\n\\n## Introduction\\nConversational assistants based on LLMs can remember the current chat with the user, and can also demonstrate in-context learning of user teachings during the conversation. But the assistant\'s memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. Then in subsequent chats the user is forced to repeat any necessary instructions over and over.\\n\\n`TeachableAgent` addresses these limitations by persisting user teachings across chat boundaries in long-term memory implemented as a vector database. Memory is automatically saved to disk at the end of each chat, then loaded from disk at the start of the next. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the teachable agent just once, and have it recall them in later chats.\\n\\nIn order to make effective decisions about memo storage and retrieval, `TeachableAgent` calls an instance of `TextAnalyzerAgent` (another AutoGen agent) to identify and reformulate text as needed for remembering facts, preferences, and skills. Note that this adds extra LLM calls involving a relatively small number of tokens, which can add a few seconds to the time a user waits for each response.\\n\\n\\n## Run It Yourself\\n\\nAutoGen contains three code examples that use `TeachableAgent`.\\n\\n1. Run [chat_with_teachable_agent.py](https://github.com/microsoft/autogen/blob/main/test/agentchat/chat_with_teachable_agent.py) to converse with `TeachableAgent`.\\n\\n2. Use the Jupyter notebook [agentchat_teachability.ipynb](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachability.ipynb) to step through examples discussed below.\\n\\n3. Run [test_teachable_agent.py](https://github.com/microsoft/autogen/blob/main/test/agentchat/test_teachable_agent.py) for quick unit testing of `TeachableAgent`.\\n\\n\\n## Basic Usage of TeachableAgent\\n1. Install dependencies\\n\\nPlease install pyautogen with the [teachable] option before using TeachableAgent.\\n```bash\\npip install \\"pyautogen[teachable]\\"\\n```\\n\\n2. Import agents\\n```python\\nfrom autogen import UserProxyAgent, config_list_from_json\\nfrom autogen.agentchat.contrib.teachable_agent import TeachableAgent\\n```\\n\\n3. Create llm_config\\n```python\\n# Load LLM inference endpoints from an env variable or a file\\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\\n# and OAI_CONFIG_LIST_sample\\nfilter_dict = {\\"model\\": [\\"gpt-4\\"]}  # GPT-3.5 is less reliable than GPT-4 at learning from user feedback.\\nconfig_list = config_list_from_json(env_or_file=\\"OAI_CONFIG_LIST\\", filter_dict=filter_dict)\\nllm_config={\\"config_list\\": config_list, \\"timeout\\": 120}\\n```\\n\\n4. Create the agents\\n```python\\nteachable_agent = TeachableAgent(\\n    name=\\"teachableagent\\",\\n    llm_config=llm_config,\\n    teach_config={\\n        \\"reset_db\\": False,  # Use True to force-reset the memo DB, and False to use an existing DB.\\n        \\"path_to_db_dir\\": \\"./tmp/interactive/teachable_agent_db\\"  # Can be any path.\\n    }\\n)\\n\\nuser = UserProxyAgent(\\"user\\", human_input_mode=\\"ALWAYS\\")\\n```\\n\\n5. Chat with `TeachableAgent`\\n```python\\n# This function will return once the user types \'exit\'.\\nteachable_agent.initiate_chat(user, message=\\"Hi, I\'m a teachable user assistant! What\'s on your mind?\\")\\n```\\n\\n6. Update the database on disk\\n```python\\n# Before closing the app, let the teachable agent store things that should be learned from this chat.\\nteachable_agent.learn_from_user_feedback()\\nteachable_agent.close_db()\\n```\\n\\n\\n## Example 1 - Learning user info\\n\\nA user can teach the agent facts about themselves.\\n(Note that due to their finetuning, LLMs can be reluctant to admit that they know personal information.)\\n```\\nCLEARING MEMORY\\nteachableagent (to user):\\n\\nHi, I\'m a teachable user assistant! What\'s on your mind?\\n\\n--------------------------------------------------------------------------------\\nProvide feedback to teachableagent. Press enter to skip and use auto-reply, or type \'exit\' to end the conversation: My name is Ricky\\nuser (to teachableagent):\\n\\nMy name is Ricky\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\nHello, Ricky! It\'s nice to meet you. What can I help you with today?\\n\\n--------------------------------------------------------------------------------\\nProvide feedback to teachableagent. Press enter to skip and use auto-reply, or type \'exit\' to end the conversation: exit\\n\\nREVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\\n\\nSAVING MEMORY TO DISK\\n    Location = ./tmp/interactive/teachable_agent_db\\\\uid_text_dict.pkl\\n```\\n\\nIn a later conversation, the user can check whether the teachable agent remembers their name. (For readability, the user prompts and some logged notices are not repeated below.)\\n```\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nwhat\'s my name?\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\nHi Ricky! How can I help you today?\\n\\n--------------------------------------------------------------------------------\\n```\\n\\n\\n## Example 2 - Learning new facts\\n\\nA user can teach the agent more complex, related facts.\\n```\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nWhat is the Vicuna model?\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\nI\'m sorry, but I don\'t have information about the \\"Vicuna model\\" in my current database. Could you please provide some context, description, or details about this model so I can better assist you?\\n\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nVicuna is a 13B-parameter language model released by Meta.\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\nThank you for providing the information. The Vicuna model is a large-scale language model developed by Meta. It consists of 13 billion parameters, which is what makes it able to understand and generate human-like text across various topics. Parameters in a neural network are the weights and biases assigned to each connection between the neurons, and having more parameters enables the model to learn and capture more complex patterns and relationships in data. As an AI language model, Vicuna can be utilized in various language-related tasks, such as text completion, translation, summarization, and more.\\n\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nWhat is the Orca model?\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\nThe Orca model is a yet-to-be-released language model by Meta, referring to the Research Preview API release of Meta\'s advanced language model. While I don\'t have specific details about the Orca model at this time, it is expected that the model would have an even larger number of parameters to further enhance its capabilities compared to its predecessor, Vicuna.\\n\\nAs AI language models evolve, they generally become more efficient at understanding and generating human-like text. This makes them more effective in completing a wide range of tasks, including text completion, translation, summarization, and Q&A, as well as providing more accurate and coherent results in various applications.\\n\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nOrca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\nThank you for the clarification. Orca is a 13-billion-parameter language model developed by Microsoft. It is designed to outperform Vicuna, the language model released by Meta. With a large number of parameters, Orca is capable of understanding and generating human-like text across diverse subjects, making it highly useful in various applications.\\n\\nOrca\'s advanced capabilities allow it to excel in tasks such as text completion, translation, summarization, Q&A, and more. Its performance improvement over Vicuna highlights the rapid advancements in natural language processing (NLP) research and the development of increasingly capable AI language models.\\n```\\n\\nThen in a later chat the teachable agent can answer questions about the facts it has been taught.\\n(Remember to first close the previous chat by typing \'exit\'.)\\n```\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nHow does the Vicuna model compare to the Orca model?\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\nThe Vicuna and Orca models are both 13B-parameter language models released by Meta and Microsoft, respectively. In terms of performance, Orca has been reported to outperform Vicuna on most tasks. However, without detailed information about specific tasks and benchmarks, it is difficult to provide a more comprehensive comparison. Generally speaking, both models are advanced language models that aim to provide high-quality natural language understanding and generation, but Orca appears to have an edge in terms of overall performance.\\n\\n--------------------------------------------------------------------------------\\n```\\n\\n\\n## Example 3 - Learning user preferences\\n\\nA user can teach the agent how they prefer to have things done.\\n\\nBe aware that a message like the next one cannot be entered as a single message through a command line because it contains a newline character.\\nSuch messages can be entered in a Jupyter notebook, or through some UI layer like that of ChatGPT.\\n```\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nPlease summarize this abstract.\\n\\nAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\\nAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\nAutoGen is an open-source framework designed to enable developers to create LLM applications with multiple customizable agents that can converse with each other to complete tasks. These agents can operate using different combinations of LLMs, human inputs, and tools, allowing developers to define agent interaction behaviors flexibly. AutoGen supports programming flexible conversation patterns using both natural language and code, making it suitable for building diverse applications with varying complexities and LLM capacities. Its effectiveness has been demonstrated through empirical studies across various domains including mathematics, coding, operations research, decision-making, and entertainment.\\n\\n--------------------------------------------------------------------------------\\nPlease summarize this abstract. When I\'m summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.\\n\\nAutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\\nAutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\n- Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\\n- Innovation: Open-source framework for creating customizable LLM applications through agent conversations, supporting various modes and interaction behaviors.\\n- Key Empirical Results: Demonstrated effectiveness across diverse application domains, including mathematics, coding, question answering, and more.\\n\\n--------------------------------------------------------------------------------\\n```\\n\\nThen in later chats the teacher doesn\'t need to reiterate their detailed preferences.\\n```\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nPlease summarize this abstract.\\n\\nSparks of Artificial General Intelligence: Early experiments with GPT-4\\nS\xe9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\\nArtificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google\'s PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4\'s performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4\'s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\n- Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4\\n- Innovation: GPT-4, an LLM with remarkable capabilities, demonstrates human-level performance across various domains, like math, coding, vision, medicine, law, and psychology.\\n- Key results: GPT-4 significantly surpasses prior models, suggesting it may be an early version of AGI; limitations and challenges toward deeper AGI are also discussed.\\n\\n--------------------------------------------------------------------------------\\n```\\n\\n\\n## Example 4 - Learning new skills\\n\\nUsers can extend the teachable agent\'s capabilities by teaching it new skills for accomplishing challenging tasks. It usually works best to first describe the task, then (in the same turn) provide a hint or advice for approaching the task.\\n\\nThe [Sparks of AGI](https://arxiv.org/abs/2303.12712) paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time. We first show a failure case, then teach the agent a strategy which lifts GPT-4\'s success rate above 95%.\\n```\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nConsider the identity:\\n9 * 4 + 6 * 6 = 72\\nCan you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\\n-Let\'s think step-by-step, write down a plan, and then write down your solution as: \\"The solution is: A * B + C * D\\".\\n\\n--------------------------------------------------------------------------------\\nStep 1: Identify the current equation and the target value on the right-hand side.\\nCurrent equation: 9 * 4 + 6 * 6 = 72\\nTarget value: 99\\n\\nStep 2: Determine what numbers can be changed on the left-hand side.\\nPossible changes: 9, 4, 6 (first occurrence), and 6 (second occurrence)\\n\\nStep 3: Evaluate whether each change in the numbers on the left-hand side can result in the target value.\\n9 -> If changed, it would create a larger product.\\n4 -> If changed, the product would increase or decrease by more than the amount needed to reach 99.\\n6 (first occurrence) -> If increased to 7, then the equation would be 9 * 4 + 7 * 6 = 94, which is still not 99.\\n6 (second occurrence) -> If increased to 7, then the equation would be 9 * 4 + 6 * 7 = 99.\\n\\nStep 4: Write down the solution.\\nThe solution is: 9 * 4 + 6 * 7\\n\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nConsider the identity:\\n9 * 4 + 6 * 6 = 72\\nCan you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\\n-Let\'s think step-by-step, write down a plan, and then write down your solution as: \\"The solution is: A * B + C * D\\".\\n\\nHere\'s some advice that may help:\\n1. Let E denote the original number on the right.\\n2. Let F denote the final number on the right.\\n3. Calculate the difference between the two, G = F - E.\\n4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\\n5. Calculate J = G / H. This is the number of times that H divides into G.\\n6. Verify that J is an integer, and that H * J = G.\\n7. Find the number on the left which is multiplied by H, and call it K.\\n8. Change K to K + J.\\n9. Recompute the value on the left, and verify that it equals F.\\nFinally, write down your solution as: \\"The solution is: A * B + C * D\\".\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\nStep 1: Identify the original number (E) and the final number (F) on the right-hand side.\\nE = 72\\nF = 99\\n\\nStep 2: Calculate the difference between the two, G = F - E.\\nG = 99 - 72 = 27\\n\\nStep 3: Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\\n9 (divides evenly into 27)\\n4 (does not)\\n6 (first occurrence, does not)\\n6 (second occurrence, does not)\\n\\nStep 4: Calculate J = G / H. This is the number of times that H divides into G.\\nH = 9 (since 9 divides evenly into 27)\\nJ = 27 / 9 = 3\\n\\nStep 5: Verify that J is an integer, and that H * J = G.\\nH * J = 9 * 3 = 27\\nJ = 3 (which is an integer)\\n\\nStep 6: Find the number on the left which is multiplied by H, and call it K.\\nNumber on the leftside of the equation that is multiplied by 9: 4\\nTherefore, K = 4\\n\\nStep 7: Change K to K + J.\\nK + J = 4 + 3 = 7\\n\\nStep 8: Recompute the value on the left, and verify that it equals F.\\n9 * 7 + 6 * 6 = 63 + 36 = 99 (which is equal to F)\\n\\nStep 9: Write down the solution.\\nThe solution is: 9 * 7 + 6 * 6\\n\\n--------------------------------------------------------------------------------\\n```\\n\\nIn a later chat the user doesn\'t need to repeat the detailed advice.\\n```\\n--------------------------------------------------------------------------------\\nuser (to teachableagent):\\n\\nConsider the identity:\\n8 * 3 + 7 * 9 = 87\\nCan you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\\n-Let\'s think step-by-step, write down a plan, and then write down your solution as: \\"The solution is: A * B + C * D\\".\\n\\n--------------------------------------------------------------------------------\\nteachableagent (to user):\\n\\nLet\'s follow the advice to modify one integer on the left hand side of the equation to make the right hand side become 59:\\n\\n1. Let E denote the original number on the right: E = 87.\\n2. Let F denote the final number on the right: F = 59.\\n3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28.\\n4. Examine the numbers on the left one by one to find one that divides evenly into G: 8, 3, 7, 9. Since 7 divides -28 evenly, we will work with 7 (H = 7).\\n5. Calculate J = G / H: J = -28 / 7 = -4.\\n6. Verify that J is an integer, and that H * J = G: J is an integer, and 7 * (-4) = -28.\\n7. Find the number on the left which is multiplied by H (7), and call it K: K = 9.\\n8. Change K to K + J: K_new = 9 + (-4) = 5.\\n9. Recompute the value on the left, and verify that it equals F: (8 * 3) + (7 * 5) = 24 + 35 = 59.\\n\\nThe solution is: 8 * 3 + 7 * 5\\n\\n--------------------------------------------------------------------------------\\n```\\n\\n\\n## Planned improvements\\n- Instructions for making any AutoGen agent user-teachable.\\n- Examples of how to include `TeachableAgent` in group chats.\\n- Expansions of AutoGen\'s current coding-testing strengths.\\n- Teachability enhancements:\\n  - Understanding user instructions distributed over multiple turns.\\n  - Learning from the agent\'s own experience, to reduce dependence on explicit user instructions.\\n  - Learning skills built on top of previously learned skills.\\n\\n\\n## Conclusion\\n\\n`TeachableAgent` is still under active research and development. For any problems you find or improvements you have in mind, please join our discussions in this repo and on our [Discord channel](https://discord.gg/pAbnFJrkgZ). We look forward to seeing how you and the rest of the community can use and improve `TeachableAgent` and the other agents in AutoGen!"},{"id":"Retrieval-Augmented Generation (RAG) Applications with AutoGen","metadata":{"permalink":"/autogen/blog/2023/10/18/RetrieveChat","source":"@site/blog/2023-10-18-RetrieveChat/index.mdx","title":"Retrieval-Augmented Generation (RAG) Applications with AutoGen","description":"RAG Architecture","date":"2023-10-18T00:00:00.000Z","formattedDate":"October 18, 2023","tags":[{"label":"LLM","permalink":"/autogen/blog/tags/llm"},{"label":"RAG","permalink":"/autogen/blog/tags/rag"}],"readingTime":9.16,"truncated":false,"authors":[{"name":"Li Jiang","title":"Senior Software Engineer at Microsoft","url":"https://github.com/thinkall","imageURL":"https://github.com/thinkall.png","key":"thinkall"}],"prevItem":{"title":"AutoGen\'s TeachableAgent","permalink":"/autogen/blog/2023/10/26/TeachableAgent"},"nextItem":{"title":"Use AutoGen for Local LLMs","permalink":"/autogen/blog/2023/07/14/Local-LLMs"}},"content":"![RAG Architecture](img/retrievechat-arch.png)\\n\\n**TL;DR:**\\n* We introduce **RetrieveUserProxyAgent** and **RetrieveAssistantAgent**, RAG agents of AutoGen that\\nallows retrieval-augmented generation, and its basic usage.\\n* We showcase customizations of RAG agents, such as customizing the embedding function, the text\\nsplit function and vector database.\\n* We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat\\napplication with Gradio.\\n\\n\\n## Introduction\\nRetrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic\\nlimitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of\\nAutoGen that allows retrieval-augmented generation. The system consists of two agents: a\\nRetrieval-augmented User Proxy agent, called `RetrieveUserProxyAgent`, and a Retrieval-augmented Assistant\\nagent, called `RetrieveAssistantAgent`, both of which are extended from built-in agents from AutoGen.\\nThe overall architecture of the RAG agents is shown in the figure above.\\n\\nTo use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented\\nUser Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy\\nnecessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented\\nUser Proxy can download the documents, segment them into chunks of a specific size, compute\\nembeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively\\nengage in code generation or question-answering adhering to the procedures outlined below:\\n1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity,\\nand sends them along with the question to the Retrieval-Augmented Assistant.\\n2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based\\non the question and context provided. If the LLM is unable to produce a satisfactory response, it\\nis instructed to reply with \u201cUpdate Context\u201d to the Retrieval-Augmented User Proxy.\\n3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and\\nsends the output as feedback. If there are no code blocks or instructions to update the context, it\\nterminates the conversation. Otherwise, it updates the context and forwards the question along\\nwith the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation\\nis enabled, individuals can proactively send any feedback, including Update Context\u201d, to the\\nRetrieval-Augmented Assistant.\\n4. If the Retrieval-Augmented Assistant receives \u201cUpdate Context\u201d, it requests the next most similar\\nchunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it\\ngenerates new code or text based on the feedback and chat history. If the LLM fails to generate\\nan answer, it replies with \u201cUpdate Context\u201d again. This process can be repeated several times.\\nThe conversation terminates if no more documents are available for the context.\\n\\n## Basic Usage of RAG Agents\\n0. Install dependencies\\n\\nPlease install pyautogen with the [retrievechat] option before using RAG agents.\\n```bash\\npip install \\"pyautogen[retrievechat]\\"\\n```\\n\\n1. Import Agents\\n```python\\nimport autogen\\nfrom autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\\nfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\\n```\\n\\n2. Create an \'RetrieveAssistantAgent\' instance named \\"assistant\\" and an \'RetrieveUserProxyAgent\' instance named \\"ragproxyagent\\"\\n```python\\nassistant = RetrieveAssistantAgent(\\n    name=\\"assistant\\",\\n    system_message=\\"You are a helpful assistant.\\",\\n    llm_config=llm_config,\\n)\\n\\nragproxyagent = RetrieveUserProxyAgent(\\n    name=\\"ragproxyagent\\",\\n    retrieve_config={\\n        \\"task\\": \\"qa\\",\\n        \\"docs_path\\": \\"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\\",\\n    },\\n)\\n```\\n\\n3. Initialize Chat and ask a question\\n```python\\nassistant.reset()\\nragproxyagent.initiate_chat(assistant, problem=\\"What is autogen?\\")\\n```\\n\\nOutput is like:\\n```\\n--------------------------------------------------------------------------------\\nassistant (to ragproxyagent):\\n\\nAutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\\n\\n--------------------------------------------------------------------------------\\n```\\n\\n4. Create a UserProxyAgent and ask the same question\\n```python\\nassistant.reset()\\nuserproxyagent = autogen.UserProxyAgent(name=\\"userproxyagent\\")\\nuserproxyagent.initiate_chat(assistant, message=\\"What is autogen?\\")\\n```\\n\\nOutput is like:\\n```\\n--------------------------------------------------------------------------------\\nassistant (to userproxyagent):\\n\\nIn computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio\'s Code Generator and Unity\'s Asset Store.\\n\\n--------------------------------------------------------------------------------\\n```\\n\\nYou can see that the output of `UserProxyAgent` is not related to our `autogen` since the latest info of\\n`autogen` is not in ChatGPT\'s training data. The output of `RetrieveUserProxyAgent` is correct as it can\\nperform retrieval-augmented generation based on the given documentation file.\\n\\n## Customizing RAG Agents\\n`RetrieveUserProxyAgent` is customizable with `retrieve_config`. There are several parameters to configure\\nbased on different use cases. In this section, we\'ll show how to customize embedding function, text split\\nfunction and vector database.\\n\\n### Customizing Embedding Function\\nBy default, [Sentence Transformers](https://www.sbert.net) and its pretrained models will be used to\\ncompute embeddings. It\'s possible that you want to use OpenAI, Cohere, HuggingFace or other embedding functions.\\n\\n* OpenAI\\n```python\\nfrom chromadb.utils import embedding_functions\\n\\nopenai_ef = embedding_functions.OpenAIEmbeddingFunction(\\n                api_key=\\"YOUR_API_KEY\\",\\n                model_name=\\"text-embedding-ada-002\\"\\n            )\\n\\nragproxyagent = RetrieveUserProxyAgent(\\n    name=\\"ragproxyagent\\",\\n    retrieve_config={\\n        \\"task\\": \\"qa\\",\\n        \\"docs_path\\": \\"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\\",\\n        \\"embedding_function\\": openai_ef,\\n    },\\n)\\n```\\n\\n* HuggingFace\\n```python\\nhuggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\\n    api_key=\\"YOUR_API_KEY\\",\\n    model_name=\\"sentence-transformers/all-MiniLM-L6-v2\\"\\n)\\n```\\n\\nMore examples can be found [here](https://docs.trychroma.com/embeddings).\\n\\n### Customizing Text Split Function\\nBefore we can store the documents into a vector database, we need to split the texts into chunks. Although\\nwe have implemented a flexible text splitter in autogen, you may still want to use different text splitters.\\nThere are also some existing text split tools which are good to reuse.\\n\\nFor example, you can use all the text splitters in langchain.\\n\\n```python\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n\\nrecur_spliter = RecursiveCharacterTextSplitter(separators=[\\"\\\\n\\", \\"\\\\r\\", \\"\\\\t\\"])\\n\\nragproxyagent = RetrieveUserProxyAgent(\\n    name=\\"ragproxyagent\\",\\n    retrieve_config={\\n        \\"task\\": \\"qa\\",\\n        \\"docs_path\\": \\"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\\",\\n        \\"custom_text_split_function\\": recur_spliter.split_text,\\n    },\\n)\\n```\\n\\n\\n### Customizing Vector Database\\nWe are using chromadb as the default vector database, you can also replace it with any other vector database\\nby simply overriding the function `retrieve_docs` of `RetrieveUserProxyAgent`.\\n\\nFor example, you can use Qdrant as below:\\n\\n```python\\n# Creating qdrant client\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(url=\\"***\\", api_key=\\"***\\")\\n\\n# Wrapping RetrieveUserProxyAgent\\nfrom litellm import embedding as test_embedding\\nfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\\nfrom qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchText\\n\\nclass QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent):\\n    def query_vector_db(\\n        self,\\n        query_texts: List[str],\\n        n_results: int = 10,\\n        search_string: str = \\"\\",\\n        **kwargs,\\n    ) -> Dict[str, Union[List[str], List[List[str]]]]:\\n        # define your own query function here\\n        embed_response = test_embedding(\'text-embedding-ada-002\', input=query_texts)\\n\\n        all_embeddings: List[List[float]] = []\\n\\n        for item in embed_response[\'data\']:\\n            all_embeddings.append(item[\'embedding\'])\\n\\n        search_queries: List[SearchRequest] = []\\n\\n        for embedding in all_embeddings:\\n            search_queries.append(\\n                SearchRequest(\\n                    vector=embedding,\\n                    filter=Filter(\\n                        must=[\\n                            FieldCondition(\\n                                key=\\"page_content\\",\\n                                match=MatchText(\\n                                    text=search_string,\\n                                )\\n                            )\\n                        ]\\n                    ),\\n                    limit=n_results,\\n                    with_payload=True,\\n                )\\n            )\\n\\n        search_response = client.search_batch(\\n            collection_name=\\"{your collection name}\\",\\n            requests=search_queries,\\n        )\\n\\n        return {\\n            \\"ids\\": [[scored_point.id for scored_point in batch] for batch in search_response],\\n            \\"documents\\": [[scored_point.payload.get(\'page_content\', \'\') for scored_point in batch] for batch in search_response],\\n            \\"metadatas\\": [[scored_point.payload.get(\'metadata\', {}) for scored_point in batch] for batch in search_response]\\n        }\\n\\n    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \\"\\", **kwargs):\\n        results = self.query_vector_db(\\n            query_texts=[problem],\\n            n_results=n_results,\\n            search_string=search_string,\\n            **kwargs,\\n        )\\n\\n        self._results = results\\n\\n\\n# Use QdrantRetrieveUserProxyAgent\\nqdrantragagent = QdrantRetrieveUserProxyAgent(\\n    name=\\"ragproxyagent\\",\\n    human_input_mode=\\"NEVER\\",\\n    max_consecutive_auto_reply=2,\\n    retrieve_config={\\n        \\"task\\": \\"qa\\",\\n    },\\n)\\n\\nqdrantragagent.retrieve_docs(\\"What is Autogen?\\", n_results=10, search_string=\\"autogen\\")\\n```\\n\\n## Advanced Usage of RAG Agents\\n### Integrate with other agents in a group chat\\nTo use `RetrieveUserProxyAgent` in a group chat is almost the same as you use it in a two agents chat. The only thing is that\\nyou need to **initialize the chat with `RetrieveUserProxyAgent`**. The `RetrieveAssistantAgent` is not necessary in a group chat.\\n\\nHowever, you may want to initialize the chat with another agent in some cases. To leverage the best of `RetrieveUserProxyAgent`,\\nyou\'ll need to call it from a function.\\n\\n```python\\nllm_config = {\\n    \\"functions\\": [\\n        {\\n            \\"name\\": \\"retrieve_content\\",\\n            \\"description\\": \\"retrieve content for code generation and question answering.\\",\\n            \\"parameters\\": {\\n                \\"type\\": \\"object\\",\\n                \\"properties\\": {\\n                    \\"message\\": {\\n                        \\"type\\": \\"string\\",\\n                        \\"description\\": \\"Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.\\",\\n                    }\\n                },\\n                \\"required\\": [\\"message\\"],\\n            },\\n        },\\n    ],\\n    \\"config_list\\": config_list,\\n    \\"timeout\\": 60,\\n    \\"seed\\": 42,\\n}\\n\\nboss = autogen.UserProxyAgent(\\n    name=\\"Boss\\",\\n    is_termination_msg=termination_msg,\\n    human_input_mode=\\"TERMINATE\\",\\n    system_message=\\"The boss who ask questions and give tasks.\\",\\n)\\n\\nboss_aid = RetrieveUserProxyAgent(\\n    name=\\"Boss_Assistant\\",\\n    is_termination_msg=termination_msg,\\n    system_message=\\"Assistant who has extra content retrieval power for solving difficult problems.\\",\\n    human_input_mode=\\"NEVER\\",\\n    max_consecutive_auto_reply=3,\\n    retrieve_config={\\n        \\"task\\": \\"qa\\",\\n    },\\n    code_execution_config=False,  # we don\'t want to execute code in this case.\\n)\\n\\ncoder = AssistantAgent(\\n    name=\\"Senior_Python_Engineer\\",\\n    is_termination_msg=termination_msg,\\n    system_message=\\"You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.\\",\\n    llm_config=llm_config,\\n)\\n\\npm = autogen.AssistantAgent(\\n    name=\\"Product_Manager\\",\\n    is_termination_msg=termination_msg,\\n    system_message=\\"You are a product manager. Reply `TERMINATE` in the end when everything is done.\\",\\n    llm_config=llm_config,\\n)\\n\\nreviewer = autogen.AssistantAgent(\\n    name=\\"Code_Reviewer\\",\\n    is_termination_msg=termination_msg,\\n    system_message=\\"You are a code reviewer. Reply `TERMINATE` in the end when everything is done.\\",\\n    llm_config=llm_config,\\n)\\n\\ndef retrieve_content(message, n_results=3):\\n        boss_aid.n_results = n_results  # Set the number of results to be retrieved.\\n        # Check if we need to update the context.\\n        update_context_case1, update_context_case2 = boss_aid._check_update_context(message)\\n        if (update_context_case1 or update_context_case2) and boss_aid.update_context:\\n            boss_aid.problem = message if not hasattr(boss_aid, \\"problem\\") else boss_aid.problem\\n            _, ret_msg = boss_aid._generate_retrieve_user_reply(message)\\n        else:\\n            ret_msg = boss_aid.generate_init_message(message, n_results=n_results)\\n        return ret_msg if ret_msg else message\\n\\nfor agent in [boss, coder, pm, reviewer]:\\n    # register functions for all agents.\\n    agent.register_function(\\n        function_map={\\n            \\"retrieve_content\\": retrieve_content,\\n        }\\n    )\\n\\ngroupchat = autogen.GroupChat(\\n    agents=[boss, coder, pm, reviewer], messages=[], max_round=12\\n)\\nmanager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\\n\\n# Start chatting with boss as this is the user proxy agent.\\nboss.initiate_chat(\\n    manager,\\n    message=\\"How to use spark for parallel training in FLAML? Give me sample code.\\",\\n)\\n```\\n\\n### Build a Chat application with Gradio\\nNow, let\'s wrap it up and make a Chat application with AutoGen and Gradio.\\n\\n![RAG ChatBot with AutoGen](img/autogen-rag.gif)\\n\\n```python\\n# Initialize Agents\\ndef initialize_agents(config_list, docs_path=None):\\n    ...\\n    return assistant, ragproxyagent\\n\\n# Initialize Chat\\ndef initiate_chat(config_list, problem, queue, n_results=3):\\n    ...\\n    assistant.reset()\\n    try:\\n        ragproxyagent.a_initiate_chat(\\n            assistant, problem=problem, silent=False, n_results=n_results\\n        )\\n        messages = ragproxyagent.chat_messages\\n        messages = [messages[k] for k in messages.keys()][0]\\n        messages = [m[\\"content\\"] for m in messages if m[\\"role\\"] == \\"user\\"]\\n        print(\\"messages: \\", messages)\\n    except Exception as e:\\n        messages = [str(e)]\\n    queue.put(messages)\\n\\n# Wrap AutoGen part into a function\\ndef chatbot_reply(input_text):\\n    \\"\\"\\"Chat with the agent through terminal.\\"\\"\\"\\n    queue = mp.Queue()\\n    process = mp.Process(\\n        target=initiate_chat,\\n        args=(config_list, input_text, queue),\\n    )\\n    process.start()\\n    try:\\n        messages = queue.get(timeout=TIMEOUT)\\n    except Exception as e:\\n        messages = [str(e) if len(str(e)) > 0 else \\"Invalid Request to OpenAI, please check your API keys.\\"]\\n    finally:\\n        try:\\n            process.terminate()\\n        except:\\n            pass\\n    return messages\\n\\n...\\n\\n# Set up UI with Gradio\\nwith gr.Blocks() as demo:\\n    ...\\n    assistant, ragproxyagent = initialize_agents(config_list)\\n\\n    chatbot = gr.Chatbot(\\n        [],\\n        elem_id=\\"chatbot\\",\\n        bubble_full_width=False,\\n        avatar_images=(None, (os.path.join(os.path.dirname(__file__), \\"autogen.png\\"))),\\n        # height=600,\\n    )\\n\\n    txt_input = gr.Textbox(\\n        scale=4,\\n        show_label=False,\\n        placeholder=\\"Enter text and press enter\\",\\n        container=False,\\n    )\\n\\n    with gr.Row():\\n        txt_model = gr.Dropdown(\\n            label=\\"Model\\",\\n            choices=[\\n                \\"gpt-4\\",\\n                \\"gpt-35-turbo\\",\\n                \\"gpt-3.5-turbo\\",\\n            ],\\n            allow_custom_value=True,\\n            value=\\"gpt-35-turbo\\",\\n            container=True,\\n        )\\n        txt_oai_key = gr.Textbox(\\n            label=\\"OpenAI API Key\\",\\n            placeholder=\\"Enter key and press enter\\",\\n            max_lines=1,\\n            show_label=True,\\n            value=os.environ.get(\\"OPENAI_API_KEY\\", \\"\\"),\\n            container=True,\\n            type=\\"password\\",\\n        )\\n        ...\\n\\n    clear = gr.ClearButton([txt_input, chatbot])\\n\\n...\\n\\nif __name__ == \\"__main__\\":\\n    demo.launch(share=True)\\n```\\n\\nThe online app and the source code are hosted in [HuggingFace](https://huggingface.co/spaces/thinkall/autogen-demos). Feel free to give it a try!\\n\\n\\n## Read More\\nYou can check out more example notebooks for RAG use cases:\\n- [Automated Code Generation and Question Answering with Retrieval Augmented Agents](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb)\\n- [Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb)"},{"id":"Use AutoGen for Local LLMs","metadata":{"permalink":"/autogen/blog/2023/07/14/Local-LLMs","source":"@site/blog/2023-07-14-Local-LLMs/index.mdx","title":"Use AutoGen for Local LLMs","description":"TL;DR:","date":"2023-07-14T00:00:00.000Z","formattedDate":"July 14, 2023","tags":[{"label":"LLM","permalink":"/autogen/blog/tags/llm"}],"readingTime":2.13,"truncated":false,"authors":[{"name":"Jiale Liu","title":"Undergraduate student at Xidian University","url":"https://leoljl.github.io","imageURL":"https://github.com/LeoLjl/leoljl.github.io/blob/main/profile.jpg?raw=true","key":"jialeliu"}],"prevItem":{"title":"Retrieval-Augmented Generation (RAG) Applications with AutoGen","permalink":"/autogen/blog/2023/10/18/RetrieveChat"},"nextItem":{"title":"MathChat - An Conversational Framework to Solve Math Problems","permalink":"/autogen/blog/2023/06/28/MathChat"}},"content":"**TL;DR:**\\nWe demonstrate how to use autogen for local LLM application. As an example, we will initiate an endpoint using [FastChat](https://github.com/lm-sys/FastChat) and perform inference on [ChatGLMv2-6b](https://github.com/THUDM/ChatGLM2-6B).\\n\\n## Preparations\\n\\n### Clone FastChat\\n\\nFastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. However, its code needs minor modification in order to function properly.\\n\\n```bash\\ngit clone https://github.com/lm-sys/FastChat.git\\ncd FastChat\\n```\\n\\n### Download checkpoint\\n\\nChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-generation version.\\n\\nBefore downloading from HuggingFace Hub, you need to have Git LFS [installed](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage).\\n\\n```bash\\ngit clone https://huggingface.co/THUDM/chatglm2-6b\\n```\\n\\n## Initiate server\\n\\nFirst, launch the controller\\n\\n```bash\\npython -m fastchat.serve.controller\\n```\\n\\nThen, launch the model worker(s)\\n\\n```bash\\npython -m fastchat.serve.model_worker --model-path chatglm2-6b\\n```\\n\\nFinally, launch the RESTful API server\\n\\n```bash\\npython -m fastchat.serve.openai_api_server --host localhost --port 8000\\n```\\n\\nNormally this will work. However, if you encounter error like [this](https://github.com/lm-sys/FastChat/issues/1641), commenting out all the lines containing `finish_reason` in `fastchat/protocol/api_protocal.py` and `fastchat/protocol/openai_api_protocol.py` will fix the problem. The modified code looks like:\\n\\n```python\\nclass CompletionResponseChoice(BaseModel):\\n    index: int\\n    text: str\\n    logprobs: Optional[int] = None\\n    # finish_reason: Optional[Literal[\\"stop\\", \\"length\\"]]\\n\\nclass CompletionResponseStreamChoice(BaseModel):\\n    index: int\\n    text: str\\n    logprobs: Optional[float] = None\\n    # finish_reason: Optional[Literal[\\"stop\\", \\"length\\"]] = None\\n```\\n\\n\\n## Interact with model using `oai.Completion` (requires openai<1)\\n\\nNow the models can be directly accessed through openai-python library as well as `autogen.oai.Completion` and `autogen.oai.ChatCompletion`.\\n\\n\\n```python\\nfrom autogen import oai\\n\\n# create a text completion request\\nresponse = oai.Completion.create(\\n    config_list=[\\n        {\\n            \\"model\\": \\"chatglm2-6b\\",\\n            \\"base_url\\": \\"http://localhost:8000/v1\\",\\n            \\"api_type\\": \\"open_ai\\",\\n            \\"api_key\\": \\"NULL\\", # just a placeholder\\n        }\\n    ],\\n    prompt=\\"Hi\\",\\n)\\nprint(response)\\n\\n# create a chat completion request\\nresponse = oai.ChatCompletion.create(\\n    config_list=[\\n        {\\n            \\"model\\": \\"chatglm2-6b\\",\\n            \\"base_url\\": \\"http://localhost:8000/v1\\",\\n            \\"api_type\\": \\"open_ai\\",\\n            \\"api_key\\": \\"NULL\\",\\n        }\\n    ],\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Hi\\"}]\\n)\\nprint(response)\\n```\\n\\nIf you would like to switch to different models, download their checkpoints and specify model path when launching model worker(s).\\n\\n## interacting with multiple local LLMs\\n\\nIf you would like to interact with multiple LLMs on your local machine, replace the `model_worker` step above with a multi model variant:\\n\\n```bash\\npython -m fastchat.serve.multi_model_worker \\\\\\n    --model-path lmsys/vicuna-7b-v1.3 \\\\\\n    --model-names vicuna-7b-v1.3 \\\\\\n    --model-path chatglm2-6b \\\\\\n    --model-names chatglm2-6b\\n```\\n\\nThe inference code would be:\\n\\n```python\\nfrom autogen import oai\\n\\n# create a chat completion request\\nresponse = oai.ChatCompletion.create(\\n    config_list=[\\n        {\\n            \\"model\\": \\"chatglm2-6b\\",\\n            \\"base_url\\": \\"http://localhost:8000/v1\\",\\n            \\"api_type\\": \\"open_ai\\",\\n            \\"api_key\\": \\"NULL\\",\\n        },\\n        {\\n            \\"model\\": \\"vicuna-7b-v1.3\\",\\n            \\"base_url\\": \\"http://localhost:8000/v1\\",\\n            \\"api_type\\": \\"open_ai\\",\\n            \\"api_key\\": \\"NULL\\",\\n        }\\n    ],\\n    messages=[{\\"role\\": \\"user\\", \\"content\\": \\"Hi\\"}]\\n)\\nprint(response)\\n```\\n\\n## For Further Reading\\n\\n* [Documentation](/docs/Getting-Started) about `autogen`.\\n* [Documentation](https://github.com/lm-sys/FastChat) about FastChat."},{"id":"MathChat - An Conversational Framework to Solve Math Problems","metadata":{"permalink":"/autogen/blog/2023/06/28/MathChat","source":"@site/blog/2023-06-28-MathChat/index.mdx","title":"MathChat - An Conversational Framework to Solve Math Problems","description":"MathChat WorkFlow","date":"2023-06-28T00:00:00.000Z","formattedDate":"June 28, 2023","tags":[{"label":"LLM","permalink":"/autogen/blog/tags/llm"},{"label":"GPT","permalink":"/autogen/blog/tags/gpt"},{"label":"research","permalink":"/autogen/blog/tags/research"}],"readingTime":7.31,"truncated":false,"authors":[{"name":"Yiran Wu","title":"PhD student at Pennsylvania State University","url":"https://github.com/kevin666aa","imageURL":"https://github.com/kevin666aa.png","key":"yiranwu"}],"prevItem":{"title":"Use AutoGen for Local LLMs","permalink":"/autogen/blog/2023/07/14/Local-LLMs"},"nextItem":{"title":"Achieve More, Pay Less - Use GPT-4 Smartly","permalink":"/autogen/blog/2023/05/18/GPT-adaptive-humaneval"}},"content":"![MathChat WorkFlow](img/mathchatflow.png)\\n**TL;DR:**\\n* **We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.**\\n* **MathChat improves LLM\'s performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.**\\n* **Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.**\\n\\nRecent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.\\n\\nIn this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.\\n\\nWe introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.\\n\\n## The MathChat Framework\\n\\nMathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.\\n\\nThe proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:\\n\\n- **Tool-using Prompt:** This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.\\n\\n- **Problem-Solving Strategy Selection Prompt:** The assistant is instructed to choose one of three potential problem-solving strategies, including:\\n  1. Writing a Python program to solve the problem directly.\\n  2. Solving the problem directly without using Python, which exercises LLMs\' inherent reasoning capabilities.\\n  3. Solving the problem step by step with Python. This should be used when the first two strategies aren\'t suitable.\\n\\n- **Final Answer Encapsulation Prompt:** This part instructs the assistant to put the final answer in *\\\\boxed*{}.\\n\\nThe prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.\\n\\nLet\'s take a look at an example between the **User Proxy Agent** and the **LLM Assistant** (GPT-4). The conversation focuses on how to solve inequality using Python.\\n(The conversation is modified for readability.)\\n\\n| Participant | Dialogue |\\n|-------------|----------|\\n| User Proxy Agent | (Initial Prompt: Let\'s solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) < (3x+9)(x+8). Express your answer in interval notation. |\\n| LLM assistant | The key idea to solve this problem is to find the critical points, which are the points where the inequality is equal. Then, test the intervals between the critical points to find the interval for which the inequality holds. |\\n| LLM assistant (continued) | Here\'s Python code to find the critical points: `from sympy import symbols, Eq, solve; x = symbols(\\"x\\"); lhs = (2*x + 10)*(x + 3); rhs = (3*x + 9)*(x + 8); equation = Eq(lhs, rhs); critical_points = solve(equation, x); critical_points` |\\n| User Proxy Agent | The critical points are: [-14, -3] |\\n| LLM assistant | We found the critical points to be x = -14 and x = -3. Now we\'ll test the intervals between these critical points as well as points before and after the critical points. The intervals are: x < -14, -14 < x < -3, x > -3. |\\n| LLM assistant (continued) | Here\'s Python code to test the inequality for each interval: `from sympy import symbols; x = symbols(\\"x\\"); inequality = (2*x + 10)*(x + 3) < (3*x + 9)*(x + 8); test_points = [-15, -5, 0]; intervals = []; for point in test_points: if inequality.subs(x, point): intervals.append(point); intervals` |\\n| User Proxy Agent | The intervals where the inequality holds true are for test points: [-15, 0] |\\n| LLM assistant | We have tested the intervals and found that the inequality holds for: 1. x < -14 (test point: -15) 2. x > -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) < (3x+9)(x+8) is x \u2208 (-\u221e, -14) \u222a (-3, \u221e) |\\n\\n## Experiment Setup\\n\\nWe evaluate the improvement brought by MathChat.\\n\\nFor the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.\\n\\nWe evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer.  For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in *\\\\boxed*{}, and we take the return of the function in PoT as the final answer.\\n\\n\\nWe also evaluate the following methods for comparison:\\n\\n1. **Vanilla prompting:** Evaluates GPT-4\'s direct problem-solving capability. The prompt used is: *\\" Solve the problem carefully. Put the final answer in \\\\boxed{}\\"*.\\n\\n2. **Program of Thoughts (PoT):** Uses a zero-shot PoT prompt that requests the model to create a *Solver* function to solve the problem and return the final answer.\\n\\n3. **Program Synthesis (PS) prompting:** Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: *\\"Write a program that answers the following question: \\\\{Problem\\\\}\\"*.\\n\\n## Experiment Results\\n\\nThe accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:\\n\\n![Result](img/result.png)\\n\\nWe found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting & Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.\\n\\nFor categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.\\n\\nThe code for experiments can be found at this [repository](https://github.com/kevin666aa/FLAML/tree/gpt_math_solver/flaml/autogen/math).\\nWe now provide an implementation of MathChat using the interactive agents in AutoGen. See this [notebook](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_MathChat.ipynb) for example usage.\\n\\n## Future Directions\\n\\nDespite MathChat\'s improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.\\n\\nFurther work can be done to enhance this framework or math problem-solving in general:\\n- Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.\\n- Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.\\n- MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.\\n\\n## For Further Reading\\n\\n* [Research paper of MathChat](https://arxiv.org/abs/2306.01337)\\n* [Documentation about `autogen`](/docs/Getting-Started)\\n\\n*Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our [Discord](https://discord.gg/pAbnFJrkgZ) server for discussion.*"},{"id":"Achieve More, Pay Less - Use GPT-4 Smartly","metadata":{"permalink":"/autogen/blog/2023/05/18/GPT-adaptive-humaneval","source":"@site/blog/2023-05-18-GPT-adaptive-humaneval/index.mdx","title":"Achieve More, Pay Less - Use GPT-4 Smartly","description":"An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding success rate and inference cost","date":"2023-05-18T00:00:00.000Z","formattedDate":"May 18, 2023","tags":[{"label":"LLM","permalink":"/autogen/blog/tags/llm"},{"label":"GPT","permalink":"/autogen/blog/tags/gpt"},{"label":"research","permalink":"/autogen/blog/tags/research"}],"readingTime":7.785,"truncated":false,"authors":[{"name":"Chi Wang","title":"Principal Researcher at Microsoft Research","url":"https://www.linkedin.com/in/chi-wang-49b15b16/","imageURL":"https://github.com/sonichi.png","key":"sonichi"}],"prevItem":{"title":"MathChat - An Conversational Framework to Solve Math Problems","permalink":"/autogen/blog/2023/06/28/MathChat"},"nextItem":{"title":"Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH","permalink":"/autogen/blog/2023/04/21/LLM-tuning-math"}},"content":"![An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding success rate and inference cost](img/humaneval.png)\\n\\n**TL;DR:**\\n* **A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.**\\n\\n\\nGPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, [HumanEval](https://huggingface.co/datasets/openai_humaneval), developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?\\n\\nIn this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.\\n\\n## Observations\\n\\n* GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.\\n* If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.\\n\\nThe obstacle of leveraging these observations is that we do not know *a priori* which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.\\n\\nTo overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let\'s look at one example code completion task:\\n\\n```python\\ndef vowels_count(s):\\n    \\"\\"\\"Write a function vowels_count which takes a string representing\\n    a word as input and returns the number of vowels in the string.\\n    Vowels in this case are \'a\', \'e\', \'i\', \'o\', \'u\'. Here, \'y\' is also a\\n    vowel, but only when it is at the end of the given word.\\n\\n    Example:\\n    >>> vowels_count(\\"abcde\\")\\n    2\\n    >>> vowels_count(\\"ACEDY\\")\\n    3\\n    \\"\\"\\"\\n```\\n\\nCan we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It\'s not obvious (but an interesting research question!) how to predict the performance without actually trying.\\n\\nWhat else can we do? We notice that:\\n**It\'s \\"easier\\" to verify a given solution than finding a correct solution from scratch.**\\n\\nSome simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.\\n\\n## Solution\\n\\nCombining these observations, we can design a solution with two intuitive ideas:\\n\\n* Make use of auto-generated feedback, i.e., code execution results, to filter responses.\\n* Try inference configurations one by one, until one response can pass the filter.\\n\\n![Design](img/design.png)\\n\\nThis solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.\\n\\nAn implementation of this solution is provided in [autogen](/docs/reference/code_utils#implement). It uses the following sequence of configurations:\\n\\n1. GPT-3.5-Turbo, n=1, temperature=0\\n1. GPT-3.5-Turbo, n=7, temperature=1, stop=[\\"\\\\nclass\\", \\"\\\\ndef\\", \\"\\\\nif\\", \\"\\\\nprint\\"]\\n1. GPT-4, n=1, temperature=0\\n1. GPT-4, n=2, temperature=1, stop=[\\"\\\\nclass\\", \\"\\\\ndef\\", \\"\\\\nif\\", \\"\\\\nprint\\"]\\n1. GPT-4, n=1, temperature=1, stop=[\\"\\\\nclass\\", \\"\\\\ndef\\", \\"\\\\nif\\", \\"\\\\nprint\\"]\\n\\n## Experiment Results\\n\\nThe first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.\\nThe inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.\\n\\nHere are a few examples of function definitions which are solved by different configurations in the portfolio.\\n\\n1. Solved by GPT-3.5-Turbo, n=1, temperature=0\\n```python\\ndef compare(game,guess):\\n    \\"\\"\\"I think we all remember that feeling when the result of some long-awaited\\n    event is finally known. The feelings and thoughts you have at that moment are\\n    definitely worth noting down and comparing.\\n    Your task is to determine if a person correctly guessed the results of a number of matches.\\n    You are given two arrays of scores and guesses of equal length, where each index shows a match.\\n    Return an array of the same length denoting how far off each guess was. If they have guessed correctly,\\n    the value is 0, and if not, the value is the absolute difference between the guess and the score.\\n\\n\\n    example:\\n\\n    compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -> [0,0,0,0,3,3]\\n    compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -> [4,4,1,0,0,6]\\n    \\"\\"\\"\\n```\\n2. Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=[\\"\\\\nclass\\", \\"\\\\ndef\\", \\"\\\\nif\\", \\"\\\\nprint\\"]: the `vowels_count` function presented earlier.\\n3. Solved by GPT-4, n=1, temperature=0:\\n```python\\ndef string_xor(a: str, b: str) -> str:\\n    \\"\\"\\" Input are two strings a and b consisting only of 1s and 0s.\\n    Perform binary XOR on these inputs and return result also as a string.\\n    >>> string_xor(\'010\', \'110\')\\n    \'100\'\\n    \\"\\"\\"\\n```\\n4. Solved by GPT-4, n=2, temperature=1, stop=[\\"\\\\nclass\\", \\"\\\\ndef\\", \\"\\\\nif\\", \\"\\\\nprint\\"]:\\n```python\\ndef is_palindrome(string: str) -> bool:\\n    \\"\\"\\" Test if given string is a palindrome \\"\\"\\"\\n    return string == string[::-1]\\n\\n\\ndef make_palindrome(string: str) -> str:\\n    \\"\\"\\" Find the shortest palindrome that begins with a supplied string.\\n    Algorithm idea is simple:\\n    - Find the longest postfix of supplied string that is a palindrome.\\n    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.\\n    >>> make_palindrome(\'\')\\n    \'\'\\n    >>> make_palindrome(\'cat\')\\n    \'catac\'\\n    >>> make_palindrome(\'cata\')\\n    \'catac\'\\n    \\"\\"\\"\\n```\\n5. Solved by GPT-4, n=1, temperature=1, stop=[\\"\\\\nclass\\", \\"\\\\ndef\\", \\"\\\\nif\\", \\"\\\\nprint\\"]:\\n```python\\ndef sort_array(arr):\\n    \\"\\"\\"\\n    In this Kata, you have to sort an array of non-negative integers according to\\n    number of ones in their binary representation in ascending order.\\n    For similar number of ones, sort based on decimal value.\\n\\n    It must be implemented like this:\\n    >>> sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]\\n    >>> sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]\\n    >>> sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]\\n    \\"\\"\\"\\n```\\n\\nThe last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:\\n* Our adaptive solution has a certain degree of fault tolerance.\\n* The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.\\n\\nIt is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.\\n\\nAn example notebook to run this experiment can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb. The experiment was run when AutoGen was a subpackage in FLAML.\\n\\n## Discussion\\n\\nOur solution is quite simple to implement using a generic interface offered in [`autogen`](/docs/Use-Cases/enhanced_inference#logic-error), yet the result is quite encouraging.\\n\\nWhile the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:\\n* Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.\\n* Consider multiple configurations to generate responses - especially useful when:\\n  - Model and other inference parameter choice affect the utility-cost tradeoff; or\\n  - Different configurations have complementary effect.\\n\\nA [previous blog post](/blog/2023/04/21/LLM-tuning-math) provides evidence that these ideas are relevant in solving math problems too.\\n`autogen` uses a technique [EcoOptiGen](https://arxiv.org/abs/2303.04673) to support inference parameter tuning and model selection.\\n\\nThere are many directions of extensions in research and development:\\n* Generalize the way to provide feedback.\\n* Automate the process of optimizing the configurations.\\n* Build adaptive agents for different applications.\\n\\n*Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our [Discord](https://discord.gg/pAbnFJrkgZ) server for discussion.*\\n\\n## For Further Reading\\n\\n* [Documentation](/docs/Getting-Started) about `autogen` and [Research paper](https://arxiv.org/abs/2303.04673).\\n* [Blog post](/blog/2023/04/21/LLM-tuning-math) about a related study for math."},{"id":"Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH","metadata":{"permalink":"/autogen/blog/2023/04/21/LLM-tuning-math","source":"@site/blog/2023-04-21-LLM-tuning-math/index.mdx","title":"Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH","description":"level 2 algebra","date":"2023-04-21T00:00:00.000Z","formattedDate":"April 21, 2023","tags":[{"label":"LLM","permalink":"/autogen/blog/tags/llm"},{"label":"GPT","permalink":"/autogen/blog/tags/gpt"},{"label":"research","permalink":"/autogen/blog/tags/research"}],"readingTime":5.015,"truncated":false,"authors":[{"name":"Chi Wang","title":"Principal Researcher at Microsoft Research","url":"https://www.linkedin.com/in/chi-wang-49b15b16/","imageURL":"https://github.com/sonichi.png","key":"sonichi"}],"prevItem":{"title":"Achieve More, Pay Less - Use GPT-4 Smartly","permalink":"/autogen/blog/2023/05/18/GPT-adaptive-humaneval"}},"content":"![level 2 algebra](img/level2algebra.png)\\n\\n**TL;DR:**\\n* **Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.**\\n* **For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.**\\n* **AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.**\\n\\n\\nLarge language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?\\n\\nIn this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for [MATH](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html), a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.\\n\\nWe will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search & pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.\\n\\nWe will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.\\n\\n## Experiment Setup\\n\\nWe use AutoGen to select between the following models with a target inference budget $0.02 per instance:\\n- gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app\\n- gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo\\n\\nWe adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:\\n\\n- temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].\\n- top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].\\n- max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].\\n- n: The number of responses to generate. We search for the optimal n in the range of [1, 100].\\n- prompt: We use the template: \\"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\\\\\\\boxed{{}}.\\" where {problem} will be replaced by the math problem instance.\\n\\nIn this experiment, when n > 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.\\n\\n## Experiment Results\\n\\nThe first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.\\n\\nSurprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.\\nThe same observation can be obtained on the level 3 Algebra test set.\\n\\n![level 3 algebra](img/level3algebra.png)\\n\\nHowever, the selected model changes on level 4 Algebra.\\n\\n![level 4 algebra](img/level4algebra.png)\\n\\nThis time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.\\nOn level 5 the result is similar.\\n\\n![level 5 algebra](img/level5algebra.png)\\n\\nWe can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.\\n\\nAn example notebook to run these experiments can be found at: https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb. The experiments were run when AutoGen was a subpackage in FLAML.\\n\\n## Analysis and Discussion\\n\\nWhile gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.\\n\\nThere are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via [`flaml.tune`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function).\\n\\nThe need for model selection, parameter tuning and cost saving is not specific to the math problems. The [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.\\n\\n## For Further Reading\\n\\n* [Research paper about the tuning technique](https://arxiv.org/abs/2303.04673)\\n* [Documentation about inference tuning](/docs/Use-Cases/enhanced_inference)\\n\\n*Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our [Discord](https://discord.gg/pAbnFJrkgZ) server for discussion.*"}]}')}}]);