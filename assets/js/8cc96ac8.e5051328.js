"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[7516],{3905:(e,t,n)=>{n.d(t,{Zo:()=>g,kt:()=>d});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},g=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),p=c(n),d=r,m=p["".concat(l,".").concat(d)]||p[d]||u[d]||i;return n?a.createElement(m,s(s({ref:t},g),{},{components:n})):a.createElement(m,s({ref:t},g))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,s=new Array(i);s[0]=p;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:r,s[1]=o;for(var c=2;c<i;c++)s[c]=n[c];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}p.displayName="MDXCreateElement"},9501:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var a=n(7462),r=(n(7294),n(3905));const i={title:"Retrieval-Augmented Generation (RAG) Applications with AutoGen",authors:"thinkall",tags:["LLM","RAG"]},s=void 0,o={permalink:"/autogen/blog/2023/10/18/RetrieveChat",source:"@site/blog/2023-10-18-RetrieveChat/index.mdx",title:"Retrieval-Augmented Generation (RAG) Applications with AutoGen",description:"RAG Architecture",date:"2023-10-18T00:00:00.000Z",formattedDate:"October 18, 2023",tags:[{label:"LLM",permalink:"/autogen/blog/tags/llm"},{label:"RAG",permalink:"/autogen/blog/tags/rag"}],readingTime:9.16,truncated:!1,authors:[{name:"Li Jiang",title:"Senior Software Engineer at Microsoft",url:"https://github.com/thinkall",imageURL:"https://github.com/thinkall.png",key:"thinkall"}],prevItem:{title:"AutoGen's TeachableAgent",permalink:"/autogen/blog/2023/10/26/TeachableAgent"},nextItem:{title:"Use AutoGen for Local LLMs",permalink:"/autogen/blog/2023/07/14/Local-LLMs"}},l={authorsImageUrls:[void 0]},c=[{value:"Introduction",id:"introduction",children:[],level:2},{value:"Basic Usage of RAG Agents",id:"basic-usage-of-rag-agents",children:[],level:2},{value:"Customizing RAG Agents",id:"customizing-rag-agents",children:[{value:"Customizing Embedding Function",id:"customizing-embedding-function",children:[],level:3},{value:"Customizing Text Split Function",id:"customizing-text-split-function",children:[],level:3},{value:"Customizing Vector Database",id:"customizing-vector-database",children:[],level:3}],level:2},{value:"Advanced Usage of RAG Agents",id:"advanced-usage-of-rag-agents",children:[{value:"Integrate with other agents in a group chat",id:"integrate-with-other-agents-in-a-group-chat",children:[],level:3},{value:"Build a Chat application with Gradio",id:"build-a-chat-application-with-gradio",children:[],level:3}],level:2},{value:"Read More",id:"read-more",children:[],level:2}],g={toc:c};function u(e){let{components:t,...i}=e;return(0,r.kt)("wrapper",(0,a.Z)({},g,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"RAG Architecture",src:n(9565).Z})),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"TL;DR:")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"We introduce ",(0,r.kt)("strong",{parentName:"li"},"RetrieveUserProxyAgent")," and ",(0,r.kt)("strong",{parentName:"li"},"RetrieveAssistantAgent"),", RAG agents of AutoGen that\nallows retrieval-augmented generation, and its basic usage."),(0,r.kt)("li",{parentName:"ul"},"We showcase customizations of RAG agents, such as customizing the embedding function, the text\nsplit function and vector database."),(0,r.kt)("li",{parentName:"ul"},"We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat\napplication with Gradio.")),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic\nlimitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of\nAutoGen that allows retrieval-augmented generation. The system consists of two agents: a\nRetrieval-augmented User Proxy agent, called ",(0,r.kt)("inlineCode",{parentName:"p"},"RetrieveUserProxyAgent"),", and a Retrieval-augmented Assistant\nagent, called ",(0,r.kt)("inlineCode",{parentName:"p"},"RetrieveAssistantAgent"),", both of which are extended from built-in agents from AutoGen.\nThe overall architecture of the RAG agents is shown in the figure above."),(0,r.kt)("p",null,"To use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented\nUser Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy\nnecessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented\nUser Proxy can download the documents, segment them into chunks of a specific size, compute\nembeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively\nengage in code generation or question-answering adhering to the procedures outlined below:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity,\nand sends them along with the question to the Retrieval-Augmented Assistant."),(0,r.kt)("li",{parentName:"ol"},"The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based\non the question and context provided. If the LLM is unable to produce a satisfactory response, it\nis instructed to reply with \u201cUpdate Context\u201d to the Retrieval-Augmented User Proxy."),(0,r.kt)("li",{parentName:"ol"},"If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and\nsends the output as feedback. If there are no code blocks or instructions to update the context, it\nterminates the conversation. Otherwise, it updates the context and forwards the question along\nwith the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation\nis enabled, individuals can proactively send any feedback, including Update Context\u201d, to the\nRetrieval-Augmented Assistant."),(0,r.kt)("li",{parentName:"ol"},"If the Retrieval-Augmented Assistant receives \u201cUpdate Context\u201d, it requests the next most similar\nchunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it\ngenerates new code or text based on the feedback and chat history. If the LLM fails to generate\nan answer, it replies with \u201cUpdate Context\u201d again. This process can be repeated several times.\nThe conversation terminates if no more documents are available for the context.")),(0,r.kt)("h2",{id:"basic-usage-of-rag-agents"},"Basic Usage of RAG Agents"),(0,r.kt)("ol",{start:0},(0,r.kt)("li",{parentName:"ol"},"Install dependencies")),(0,r.kt)("p",null,"Please install pyautogen with the ","[retrievechat]"," option before using RAG agents."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'pip install "pyautogen[retrievechat]"\n')),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Import Agents")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"import autogen\nfrom autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\nfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n")),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Create an 'RetrieveAssistantAgent' instance named \"assistant\" and an 'RetrieveUserProxyAgent' instance named \"ragproxyagent\"")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'assistant = RetrieveAssistantAgent(\n    name="assistant",\n    system_message="You are a helpful assistant.",\n    llm_config=llm_config,\n)\n\nragproxyagent = RetrieveUserProxyAgent(\n    name="ragproxyagent",\n    retrieve_config={\n        "task": "qa",\n        "docs_path": "https://raw.githubusercontent.com/microsoft/autogen/main/README.md",\n    },\n)\n')),(0,r.kt)("ol",{start:3},(0,r.kt)("li",{parentName:"ol"},"Initialize Chat and ask a question")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'assistant.reset()\nragproxyagent.initiate_chat(assistant, problem="What is autogen?")\n')),(0,r.kt)("p",null,"Output is like:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"--------------------------------------------------------------------------------\nassistant (to ragproxyagent):\n\nAutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\n\n--------------------------------------------------------------------------------\n")),(0,r.kt)("ol",{start:4},(0,r.kt)("li",{parentName:"ol"},"Create a UserProxyAgent and ask the same question")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'assistant.reset()\nuserproxyagent = autogen.UserProxyAgent(name="userproxyagent")\nuserproxyagent.initiate_chat(assistant, message="What is autogen?")\n')),(0,r.kt)("p",null,"Output is like:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"--------------------------------------------------------------------------------\nassistant (to userproxyagent):\n\nIn computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio's Code Generator and Unity's Asset Store.\n\n--------------------------------------------------------------------------------\n")),(0,r.kt)("p",null,"You can see that the output of ",(0,r.kt)("inlineCode",{parentName:"p"},"UserProxyAgent")," is not related to our ",(0,r.kt)("inlineCode",{parentName:"p"},"autogen")," since the latest info of\n",(0,r.kt)("inlineCode",{parentName:"p"},"autogen")," is not in ChatGPT's training data. The output of ",(0,r.kt)("inlineCode",{parentName:"p"},"RetrieveUserProxyAgent")," is correct as it can\nperform retrieval-augmented generation based on the given documentation file."),(0,r.kt)("h2",{id:"customizing-rag-agents"},"Customizing RAG Agents"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"RetrieveUserProxyAgent")," is customizable with ",(0,r.kt)("inlineCode",{parentName:"p"},"retrieve_config"),". There are several parameters to configure\nbased on different use cases. In this section, we'll show how to customize embedding function, text split\nfunction and vector database."),(0,r.kt)("h3",{id:"customizing-embedding-function"},"Customizing Embedding Function"),(0,r.kt)("p",null,"By default, ",(0,r.kt)("a",{parentName:"p",href:"https://www.sbert.net"},"Sentence Transformers")," and its pretrained models will be used to\ncompute embeddings. It's possible that you want to use OpenAI, Cohere, HuggingFace or other embedding functions."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"OpenAI")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from chromadb.utils import embedding_functions\n\nopenai_ef = embedding_functions.OpenAIEmbeddingFunction(\n                api_key="YOUR_API_KEY",\n                model_name="text-embedding-ada-002"\n            )\n\nragproxyagent = RetrieveUserProxyAgent(\n    name="ragproxyagent",\n    retrieve_config={\n        "task": "qa",\n        "docs_path": "https://raw.githubusercontent.com/microsoft/autogen/main/README.md",\n        "embedding_function": openai_ef,\n    },\n)\n')),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"HuggingFace")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n    api_key="YOUR_API_KEY",\n    model_name="sentence-transformers/all-MiniLM-L6-v2"\n)\n')),(0,r.kt)("p",null,"More examples can be found ",(0,r.kt)("a",{parentName:"p",href:"https://docs.trychroma.com/embeddings"},"here"),"."),(0,r.kt)("h3",{id:"customizing-text-split-function"},"Customizing Text Split Function"),(0,r.kt)("p",null,"Before we can store the documents into a vector database, we need to split the texts into chunks. Although\nwe have implemented a flexible text splitter in autogen, you may still want to use different text splitters.\nThere are also some existing text split tools which are good to reuse."),(0,r.kt)("p",null,"For example, you can use all the text splitters in langchain."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.text_splitter import RecursiveCharacterTextSplitter\n\nrecur_spliter = RecursiveCharacterTextSplitter(separators=["\\n", "\\r", "\\t"])\n\nragproxyagent = RetrieveUserProxyAgent(\n    name="ragproxyagent",\n    retrieve_config={\n        "task": "qa",\n        "docs_path": "https://raw.githubusercontent.com/microsoft/autogen/main/README.md",\n        "custom_text_split_function": recur_spliter.split_text,\n    },\n)\n')),(0,r.kt)("h3",{id:"customizing-vector-database"},"Customizing Vector Database"),(0,r.kt)("p",null,"We are using chromadb as the default vector database, you can also replace it with any other vector database\nby simply overriding the function ",(0,r.kt)("inlineCode",{parentName:"p"},"retrieve_docs")," of ",(0,r.kt)("inlineCode",{parentName:"p"},"RetrieveUserProxyAgent"),"."),(0,r.kt)("p",null,"For example, you can use Qdrant as below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# Creating qdrant client\nfrom qdrant_client import QdrantClient\n\nclient = QdrantClient(url="***", api_key="***")\n\n# Wrapping RetrieveUserProxyAgent\nfrom litellm import embedding as test_embedding\nfrom autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\nfrom qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchText\n\nclass QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n    def query_vector_db(\n        self,\n        query_texts: List[str],\n        n_results: int = 10,\n        search_string: str = "",\n        **kwargs,\n    ) -> Dict[str, Union[List[str], List[List[str]]]]:\n        # define your own query function here\n        embed_response = test_embedding(\'text-embedding-ada-002\', input=query_texts)\n\n        all_embeddings: List[List[float]] = []\n\n        for item in embed_response[\'data\']:\n            all_embeddings.append(item[\'embedding\'])\n\n        search_queries: List[SearchRequest] = []\n\n        for embedding in all_embeddings:\n            search_queries.append(\n                SearchRequest(\n                    vector=embedding,\n                    filter=Filter(\n                        must=[\n                            FieldCondition(\n                                key="page_content",\n                                match=MatchText(\n                                    text=search_string,\n                                )\n                            )\n                        ]\n                    ),\n                    limit=n_results,\n                    with_payload=True,\n                )\n            )\n\n        search_response = client.search_batch(\n            collection_name="{your collection name}",\n            requests=search_queries,\n        )\n\n        return {\n            "ids": [[scored_point.id for scored_point in batch] for batch in search_response],\n            "documents": [[scored_point.payload.get(\'page_content\', \'\') for scored_point in batch] for batch in search_response],\n            "metadatas": [[scored_point.payload.get(\'metadata\', {}) for scored_point in batch] for batch in search_response]\n        }\n\n    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = "", **kwargs):\n        results = self.query_vector_db(\n            query_texts=[problem],\n            n_results=n_results,\n            search_string=search_string,\n            **kwargs,\n        )\n\n        self._results = results\n\n\n# Use QdrantRetrieveUserProxyAgent\nqdrantragagent = QdrantRetrieveUserProxyAgent(\n    name="ragproxyagent",\n    human_input_mode="NEVER",\n    max_consecutive_auto_reply=2,\n    retrieve_config={\n        "task": "qa",\n    },\n)\n\nqdrantragagent.retrieve_docs("What is Autogen?", n_results=10, search_string="autogen")\n')),(0,r.kt)("h2",{id:"advanced-usage-of-rag-agents"},"Advanced Usage of RAG Agents"),(0,r.kt)("h3",{id:"integrate-with-other-agents-in-a-group-chat"},"Integrate with other agents in a group chat"),(0,r.kt)("p",null,"To use ",(0,r.kt)("inlineCode",{parentName:"p"},"RetrieveUserProxyAgent")," in a group chat is almost the same as you use it in a two agents chat. The only thing is that\nyou need to ",(0,r.kt)("strong",{parentName:"p"},"initialize the chat with ",(0,r.kt)("inlineCode",{parentName:"strong"},"RetrieveUserProxyAgent")),". The ",(0,r.kt)("inlineCode",{parentName:"p"},"RetrieveAssistantAgent")," is not necessary in a group chat."),(0,r.kt)("p",null,"However, you may want to initialize the chat with another agent in some cases. To leverage the best of ",(0,r.kt)("inlineCode",{parentName:"p"},"RetrieveUserProxyAgent"),",\nyou'll need to call it from a function."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'llm_config = {\n    "functions": [\n        {\n            "name": "retrieve_content",\n            "description": "retrieve content for code generation and question answering.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "message": {\n                        "type": "string",\n                        "description": "Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.",\n                    }\n                },\n                "required": ["message"],\n            },\n        },\n    ],\n    "config_list": config_list,\n    "timeout": 60,\n    "seed": 42,\n}\n\nboss = autogen.UserProxyAgent(\n    name="Boss",\n    is_termination_msg=termination_msg,\n    human_input_mode="TERMINATE",\n    system_message="The boss who ask questions and give tasks.",\n)\n\nboss_aid = RetrieveUserProxyAgent(\n    name="Boss_Assistant",\n    is_termination_msg=termination_msg,\n    system_message="Assistant who has extra content retrieval power for solving difficult problems.",\n    human_input_mode="NEVER",\n    max_consecutive_auto_reply=3,\n    retrieve_config={\n        "task": "qa",\n    },\n    code_execution_config=False,  # we don\'t want to execute code in this case.\n)\n\ncoder = AssistantAgent(\n    name="Senior_Python_Engineer",\n    is_termination_msg=termination_msg,\n    system_message="You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.",\n    llm_config=llm_config,\n)\n\npm = autogen.AssistantAgent(\n    name="Product_Manager",\n    is_termination_msg=termination_msg,\n    system_message="You are a product manager. Reply `TERMINATE` in the end when everything is done.",\n    llm_config=llm_config,\n)\n\nreviewer = autogen.AssistantAgent(\n    name="Code_Reviewer",\n    is_termination_msg=termination_msg,\n    system_message="You are a code reviewer. Reply `TERMINATE` in the end when everything is done.",\n    llm_config=llm_config,\n)\n\ndef retrieve_content(message, n_results=3):\n        boss_aid.n_results = n_results  # Set the number of results to be retrieved.\n        # Check if we need to update the context.\n        update_context_case1, update_context_case2 = boss_aid._check_update_context(message)\n        if (update_context_case1 or update_context_case2) and boss_aid.update_context:\n            boss_aid.problem = message if not hasattr(boss_aid, "problem") else boss_aid.problem\n            _, ret_msg = boss_aid._generate_retrieve_user_reply(message)\n        else:\n            ret_msg = boss_aid.generate_init_message(message, n_results=n_results)\n        return ret_msg if ret_msg else message\n\nfor agent in [boss, coder, pm, reviewer]:\n    # register functions for all agents.\n    agent.register_function(\n        function_map={\n            "retrieve_content": retrieve_content,\n        }\n    )\n\ngroupchat = autogen.GroupChat(\n    agents=[boss, coder, pm, reviewer], messages=[], max_round=12\n)\nmanager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n\n# Start chatting with boss as this is the user proxy agent.\nboss.initiate_chat(\n    manager,\n    message="How to use spark for parallel training in FLAML? Give me sample code.",\n)\n')),(0,r.kt)("h3",{id:"build-a-chat-application-with-gradio"},"Build a Chat application with Gradio"),(0,r.kt)("p",null,"Now, let's wrap it up and make a Chat application with AutoGen and Gradio."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"RAG ChatBot with AutoGen",src:n(104).Z})),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'# Initialize Agents\ndef initialize_agents(config_list, docs_path=None):\n    ...\n    return assistant, ragproxyagent\n\n# Initialize Chat\ndef initiate_chat(config_list, problem, queue, n_results=3):\n    ...\n    assistant.reset()\n    try:\n        ragproxyagent.a_initiate_chat(\n            assistant, problem=problem, silent=False, n_results=n_results\n        )\n        messages = ragproxyagent.chat_messages\n        messages = [messages[k] for k in messages.keys()][0]\n        messages = [m["content"] for m in messages if m["role"] == "user"]\n        print("messages: ", messages)\n    except Exception as e:\n        messages = [str(e)]\n    queue.put(messages)\n\n# Wrap AutoGen part into a function\ndef chatbot_reply(input_text):\n    """Chat with the agent through terminal."""\n    queue = mp.Queue()\n    process = mp.Process(\n        target=initiate_chat,\n        args=(config_list, input_text, queue),\n    )\n    process.start()\n    try:\n        messages = queue.get(timeout=TIMEOUT)\n    except Exception as e:\n        messages = [str(e) if len(str(e)) > 0 else "Invalid Request to OpenAI, please check your API keys."]\n    finally:\n        try:\n            process.terminate()\n        except:\n            pass\n    return messages\n\n...\n\n# Set up UI with Gradio\nwith gr.Blocks() as demo:\n    ...\n    assistant, ragproxyagent = initialize_agents(config_list)\n\n    chatbot = gr.Chatbot(\n        [],\n        elem_id="chatbot",\n        bubble_full_width=False,\n        avatar_images=(None, (os.path.join(os.path.dirname(__file__), "autogen.png"))),\n        # height=600,\n    )\n\n    txt_input = gr.Textbox(\n        scale=4,\n        show_label=False,\n        placeholder="Enter text and press enter",\n        container=False,\n    )\n\n    with gr.Row():\n        txt_model = gr.Dropdown(\n            label="Model",\n            choices=[\n                "gpt-4",\n                "gpt-35-turbo",\n                "gpt-3.5-turbo",\n            ],\n            allow_custom_value=True,\n            value="gpt-35-turbo",\n            container=True,\n        )\n        txt_oai_key = gr.Textbox(\n            label="OpenAI API Key",\n            placeholder="Enter key and press enter",\n            max_lines=1,\n            show_label=True,\n            value=os.environ.get("OPENAI_API_KEY", ""),\n            container=True,\n            type="password",\n        )\n        ...\n\n    clear = gr.ClearButton([txt_input, chatbot])\n\n...\n\nif __name__ == "__main__":\n    demo.launch(share=True)\n')),(0,r.kt)("p",null,"The online app and the source code are hosted in ",(0,r.kt)("a",{parentName:"p",href:"https://huggingface.co/spaces/thinkall/autogen-demos"},"HuggingFace"),". Feel free to give it a try!"),(0,r.kt)("h2",{id:"read-more"},"Read More"),(0,r.kt)("p",null,"You can check out more example notebooks for RAG use cases:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb"},"Automated Code Generation and Question Answering with Retrieval Augmented Agents")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb"},"Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)"))))}u.isMDXComponent=!0},104:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/autogen-rag-ee3b1d222f3e10b1707527cdea69be50.gif"},9565:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/retrievechat-arch-959e180405c99ceb3da88a441c02f45e.png"}}]);