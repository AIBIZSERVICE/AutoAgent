"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4288],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>m});var o=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,o,i=function(e,n){if(null==e)return{};var t,o,i={},a=Object.keys(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)t=a[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=o.createContext({}),p=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},u=function(e){var n=p(e.components);return o.createElement(s.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},c=o.forwardRef((function(e,n){var t=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),c=p(t),m=i,h=c["".concat(s,".").concat(m)]||c[m]||d[m]||a;return t?o.createElement(h,r(r({ref:n},u),{},{components:t})):o.createElement(h,r({ref:n},u))}));function m(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var a=t.length,r=new Array(a);r[0]=c;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,r[1]=l;for(var p=2;p<a;p++)r[p]=t[p];return o.createElement.apply(null,r)}return o.createElement.apply(null,t)}c.displayName="MDXCreateElement"},3581:(e,n,t)=>{t.r(n),t.d(n,{contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>l,toc:()=>s});var o=t(7462),i=(t(7294),t(3905));const a={},r="Frequently Asked Questions",l={unversionedId:"FAQ",id:"FAQ",isDocsHomePage:!1,title:"Frequently Asked Questions",description:"Set your API endpoints",source:"@site/docs/FAQ.md",sourceDirName:".",slug:"/FAQ",permalink:"/autogen/docs/FAQ",editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/FAQ.md",tags:[],version:"current",frontMatter:{}},s=[{value:"Set your API endpoints",id:"set-your-api-endpoints",children:[{value:"Option 1: Load a list of endpoints from json",id:"option-1-load-a-list-of-endpoints-from-json",children:[],level:3},{value:"Option 2: Construct a list of endpoints for OpenAI or Azure OpenAI",id:"option-2-construct-a-list-of-endpoints-for-openai-or-azure-openai",children:[],level:3},{value:"Use the constructed configuration list in agents",id:"use-the-constructed-configuration-list-in-agents",children:[],level:3},{value:"Can I use non-OpenAI models?",id:"can-i-use-non-openai-models",children:[],level:3}],level:2},{value:"Handle Rate Limit Error and Timeout Error",id:"handle-rate-limit-error-and-timeout-error",children:[],level:2},{value:"How to continue a finished conversation",id:"how-to-continue-a-finished-conversation",children:[],level:2},{value:"How do we decide what LLM is used for each agent? How many agents can be used? How do we decide how many agents in the group?",id:"how-do-we-decide-what-llm-is-used-for-each-agent-how-many-agents-can-be-used-how-do-we-decide-how-many-agents-in-the-group",children:[],level:2},{value:"Why is code not saved as file?",id:"why-is-code-not-saved-as-file",children:[],level:2},{value:"Code execution",id:"code-execution",children:[{value:"Enable Python 3 docker image",id:"enable-python-3-docker-image",children:[],level:3}],level:2}],p={toc:s};function u(e){let{components:n,...t}=e;return(0,i.kt)("wrapper",(0,o.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"frequently-asked-questions"},"Frequently Asked Questions"),(0,i.kt)("h2",{id:"set-your-api-endpoints"},"Set your API endpoints"),(0,i.kt)("p",null,"There are multiple ways to construct a list of configurations for LLM inference."),(0,i.kt)("h3",{id:"option-1-load-a-list-of-endpoints-from-json"},"Option 1: Load a list of endpoints from json"),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"/docs/reference/oai/openai_utils#config_list_from_json"},(0,i.kt)("inlineCode",{parentName:"a"},"config_list_from_json"))," function loads a list of configurations from an environment variable or a json file."),(0,i.kt)("p",null,"For example,"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import autogen\nconfig_list = autogen.config_list_from_json(\n    "OAI_CONFIG_LIST",\n    file_location=".",\n    filter_dict={\n        "model": {\n            "gpt-4",\n            "gpt-3.5-turbo",\n        }\n    }\n)\n')),(0,i.kt)("p",null,'It first looks for environment variable "OAI_CONFIG_LIST" which needs to be a valid json string. If that variable is not found, it then looks for a json file named "OAI_CONFIG_LIST" under the specified ',(0,i.kt)("inlineCode",{parentName:"p"},"file_location"),". It then filters the configs by models (you can filter by other keys as well)."),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"OAI_CONFIG_LIST")," var or file content looks like the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-json"},'[\n    {\n        "model": "gpt-4",\n        "api_key": "<your OpenAI API key here>"\n    },\n    {\n        "model": "gpt-4",\n        "api_key": "<your Azure OpenAI API key here>",\n        "api_base": "<your Azure OpenAI API base here>",\n        "api_type": "azure",\n        "api_version": "2023-07-01-preview"\n    },\n    {\n        "model": "gpt-3.5-turbo",\n        "api_key": "<your Azure OpenAI API key here>",\n        "api_base": "<your Azure OpenAI API base here>",\n        "api_type": "azure",\n        "api_version": "2023-07-01-preview"\n    }\n]\n')),(0,i.kt)("h3",{id:"option-2-construct-a-list-of-endpoints-for-openai-or-azure-openai"},"Option 2: Construct a list of endpoints for OpenAI or Azure OpenAI"),(0,i.kt)("p",null,"The ",(0,i.kt)("a",{parentName:"p",href:"/docs/reference/oai/openai_utils#config_list_from_models"},(0,i.kt)("inlineCode",{parentName:"a"},"config_list_from_models"))," function tries to create a list of configurations using Azure OpenAI endpoints and OpenAI endpoints for the provided list of models. It assumes the api keys and api bases are stored in the corresponding environment variables or local txt files:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"OpenAI API key: os.environ",'["OPENAI_API_KEY"]'," or ",(0,i.kt)("inlineCode",{parentName:"li"},'openai_api_key_file="key_openai.txt"'),"."),(0,i.kt)("li",{parentName:"ul"},"Azure OpenAI API key: os.environ",'["AZURE_OPENAI_API_KEY"]'," or ",(0,i.kt)("inlineCode",{parentName:"li"},'aoai_api_key_file="key_aoai.txt"'),". Multiple keys can be stored, one per line."),(0,i.kt)("li",{parentName:"ul"},"Azure OpenAI API base: os.environ",'["AZURE_OPENAI_API_BASE"]'," or ",(0,i.kt)("inlineCode",{parentName:"li"},'aoai_api_base_file="base_aoai.txt"'),". Multiple bases can be stored, one per line.")),(0,i.kt)("p",null,"It's OK to have only the OpenAI API key, or only the Azure OpenAI API key + base."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import autogen\nconfig_list = autogen.config_list_from_models(model_list=["gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-16k"])\n')),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"For Azure the model name refers to the OpenAI Studio deployment name.")),(0,i.kt)("p",null,"The config list looks like the following, if only OpenAI API key is available:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"config_list = [\n    {\n        'model': 'gpt-4',\n        'api_key': '<your OpenAI API key here>',\n    },  # OpenAI API endpoint for gpt-4\n    {\n        'model': 'gpt-3.5-turbo',\n        'api_key': '<your OpenAI API key here>',\n    },  # OpenAI API endpoint for gpt-3.5-turbo\n    {\n        'model': 'gpt-3.5-turbo-16k',\n        'api_key': '<your OpenAI API key here>',\n    },  # OpenAI API endpoint for gpt-3.5-turbo-16k\n]\n")),(0,i.kt)("h3",{id:"use-the-constructed-configuration-list-in-agents"},"Use the constructed configuration list in agents"),(0,i.kt)("p",null,'Make sure the "config_list" is included in the ',(0,i.kt)("inlineCode",{parentName:"p"},"llm_config")," in the constructor of the LLM-based agent. For example,"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'assistant = autogen.AssistantAgent(\n    name="assistant",\n    llm_config={"config_list": config_list}\n)\n')),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"llm_config")," is used in the ",(0,i.kt)("a",{parentName:"p",href:"/docs/reference/oai/completion#create"},(0,i.kt)("inlineCode",{parentName:"a"},"create"))," function for LLM inference.\nWhen ",(0,i.kt)("inlineCode",{parentName:"p"},"llm_config")," is not provided, the agent will rely on other openai settings such as ",(0,i.kt)("inlineCode",{parentName:"p"},"openai.api_key")," or the environment variable ",(0,i.kt)("inlineCode",{parentName:"p"},"OPENAI_API_KEY"),", which can also work when you'd like to use a single endpoint.\nYou can also explicitly specify that by:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'assistant = autogen.AssistantAgent(name="assistant", llm_config={"api_key": ...})\n')),(0,i.kt)("h3",{id:"can-i-use-non-openai-models"},"Can I use non-OpenAI models?"),(0,i.kt)("p",null,"Yes. Please check ",(0,i.kt)("a",{parentName:"p",href:"https://microsoft.github.io/autogen/blog/2023/07/14/Local-LLMs"},"https://microsoft.github.io/autogen/blog/2023/07/14/Local-LLMs")," for an example."),(0,i.kt)("h2",{id:"handle-rate-limit-error-and-timeout-error"},"Handle Rate Limit Error and Timeout Error"),(0,i.kt)("p",null,"You can set ",(0,i.kt)("inlineCode",{parentName:"p"},"retry_wait_time")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"max_retry_period")," to handle rate limit error. And you can set ",(0,i.kt)("inlineCode",{parentName:"p"},"request_timeout")," to handle timeout error. They can all be specified in ",(0,i.kt)("inlineCode",{parentName:"p"},"llm_config")," for an agent, which will be used in the ",(0,i.kt)("a",{parentName:"p",href:"/docs/reference/oai/completion#create"},(0,i.kt)("inlineCode",{parentName:"a"},"create"))," function for LLM inference."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"retry_wait_time")," (int): the time interval to wait (in seconds) before retrying a failed request."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"max_retry_period")," (int): the total timeout (in seconds) allowed for retrying failed requests."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"request_timeout")," (int): the timeout (in seconds) sent with a single request.")),(0,i.kt)("p",null,"Please refer to the ",(0,i.kt)("a",{parentName:"p",href:"/docs/Use-Cases/enhanced_inference#runtime-error"},"documentation")," for more info."),(0,i.kt)("h2",{id:"how-to-continue-a-finished-conversation"},"How to continue a finished conversation"),(0,i.kt)("p",null,"When you call ",(0,i.kt)("inlineCode",{parentName:"p"},"initiate_chat")," the conversation restarts by default. You can use ",(0,i.kt)("inlineCode",{parentName:"p"},"send")," or ",(0,i.kt)("inlineCode",{parentName:"p"},"initiate_chat(clear_history=False)")," to continue the conversation."),(0,i.kt)("h2",{id:"how-do-we-decide-what-llm-is-used-for-each-agent-how-many-agents-can-be-used-how-do-we-decide-how-many-agents-in-the-group"},"How do we decide what LLM is used for each agent? How many agents can be used? How do we decide how many agents in the group?"),(0,i.kt)("p",null,"Each agent can be customized. You can use LLMs, tools or human behind each agent. If you use an LLM for an agent, use the one best suited for its role. There is no limit of the number of agents, but start from a small number like 2, 3. The more capable is the LLM and the fewer roles you need, the fewer agents you need."),(0,i.kt)("p",null,"The default user proxy agent doesn't use LLM. If you'd like to use an LLM in UserProxyAgent, the use case could be to simulate user's behavior."),(0,i.kt)("p",null,"The default assistant agent is instructed to use both coding and language skills. It doesn't have to do coding, depending on the tasks. And you can customize the system message. So if you want to use it for coding, use a model that's good at coding."),(0,i.kt)("h2",{id:"why-is-code-not-saved-as-file"},"Why is code not saved as file?"),(0,i.kt)("p",null,"If you are using a custom system message for the coding agent, please include something like:\n",(0,i.kt)("inlineCode",{parentName:"p"},"If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line."),"\nin the system message. This line is in the default system message of the ",(0,i.kt)("inlineCode",{parentName:"p"},"AssistantAgent"),"."),(0,i.kt)("p",null,"If the ",(0,i.kt)("inlineCode",{parentName:"p"},"# filename"),' doesn\'t appear in the suggested code still, consider adding explicit instructions such as "save the code to disk" in the initial user message in ',(0,i.kt)("inlineCode",{parentName:"p"},"initiate_chat"),".\nThe ",(0,i.kt)("inlineCode",{parentName:"p"},"AssistantAgent")," doesn't save all the code by default, because there are cases in which one would just like to finish a task without saving the code."),(0,i.kt)("h2",{id:"code-execution"},"Code execution"),(0,i.kt)("p",null,"We strongly recommend using docker to execute code. There are two ways to use docker:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Run autogen in a docker container. For example, when developing in GitHub codespace, the autogen runs in a docker container."),(0,i.kt)("li",{parentName:"ol"},"Run autogen outside of a docker, while perform code execution with a docker container. For this option, make sure the python package ",(0,i.kt)("inlineCode",{parentName:"li"},"docker")," is installed. When it is not installed and ",(0,i.kt)("inlineCode",{parentName:"li"},"use_docker")," is omitted in ",(0,i.kt)("inlineCode",{parentName:"li"},"code_execution_config"),", the code will be executed locally (this behavior is subject to change in future).")),(0,i.kt)("h3",{id:"enable-python-3-docker-image"},"Enable Python 3 docker image"),(0,i.kt)("p",null,"You might want to override the default docker image used for code execution. To do that set ",(0,i.kt)("inlineCode",{parentName:"p"},"use_docker")," key of ",(0,i.kt)("inlineCode",{parentName:"p"},"code_execution_config")," property to the name of the image. E.g.:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'user_proxy = autogen.UserProxyAgent(\n    name="agent",\n    human_input_mode="TERMINATE",\n    max_consecutive_auto_reply=10,\n    code_execution_config={"work_dir":"_output", "use_docker":"python:3"},\n    llm_config=llm_config,\n    system_message=""""Reply TERMINATE if the task has been solved at full satisfaction.\nOtherwise, reply CONTINUE, or the reason why the task is not solved yet."""\n)\n')),(0,i.kt)("p",null,"If you have problems with agents running ",(0,i.kt)("inlineCode",{parentName:"p"},"pip install")," or get errors similar to ",(0,i.kt)("inlineCode",{parentName:"p"},"Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory')"),", you can choose ",(0,i.kt)("strong",{parentName:"p"},"'python:3'")," as image as shown in the code example above and that should solve the problem."))}u.isMDXComponent=!0}}]);